{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper: Link Prediction Based on Graph Neural Networks (NeurIPS 2018)\n",
    "# Example: https://github.com/rusty1s/pytorch_geometric/blob/99a496e077a4d41417c7d927df7730fd984004b9/examples/seal_link_pred.py#L90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "large-costs",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os.path as osp\n",
    "from itertools import chain\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.sparse.csgraph import shortest_path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from torch.nn import ModuleList, Linear, Conv1d, MaxPool1d\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import GCNConv, global_sort_pool\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader, Dataset\n",
    "from torch_geometric.utils import (negative_sampling, add_self_loops,\n",
    "                                   train_test_split_edges, k_hop_subgraph,\n",
    "                                   to_scipy_sparse_matrix, to_undirected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-forum",
   "metadata": {},
   "source": [
    "# Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "spatial-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEALDataset(InMemoryDataset):\n",
    "    def __init__(self, dataset, num_hops, split='train'):\n",
    "        self.data = dataset[0]\n",
    "        self.num_hops = num_hops\n",
    "        super(SEALDataset, self).__init__(dataset.root)\n",
    "        index = ['train', 'val', 'test'].index(split)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[index])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_train_data.pt', 'SEAL_val_data.pt', 'SEAL_test_data.pt']\n",
    "\n",
    "    def process(self):\n",
    "        random.seed(12345)\n",
    "        torch.manual_seed(12345)\n",
    "\n",
    "        data = train_test_split_edges(self.data)\n",
    "\n",
    "        edge_index, _ = add_self_loops(data.train_pos_edge_index)\n",
    "        \n",
    "        data.train_neg_edge_index = negative_sampling(\n",
    "            edge_index, num_nodes=data.num_nodes,\n",
    "            num_neg_samples=data.train_pos_edge_index.size(1))\n",
    "\n",
    "        self.__max_z__ = 0\n",
    "\n",
    "        # Collect a list of subgraphs for training, validation and test.\n",
    "        train_pos_list = self.extract_enclosing_subgraphs(\n",
    "            data.train_pos_edge_index, data.train_pos_edge_index, 1)\n",
    "        train_neg_list = self.extract_enclosing_subgraphs(\n",
    "            data.train_neg_edge_index, data.train_pos_edge_index, 0)\n",
    "\n",
    "        val_pos_list = self.extract_enclosing_subgraphs(\n",
    "            data.val_pos_edge_index, data.train_pos_edge_index, 1)\n",
    "        val_neg_list = self.extract_enclosing_subgraphs(\n",
    "            data.val_neg_edge_index, data.train_pos_edge_index, 0)\n",
    "\n",
    "        test_pos_list = self.extract_enclosing_subgraphs(\n",
    "            data.test_pos_edge_index, data.train_pos_edge_index, 1)\n",
    "        test_neg_list = self.extract_enclosing_subgraphs(\n",
    "            data.test_neg_edge_index, data.train_pos_edge_index, 0)\n",
    "\n",
    "        # Convert labels to one-hot features.\n",
    "        for data in chain(train_pos_list, train_neg_list, val_pos_list,\n",
    "                          val_neg_list, test_pos_list, test_neg_list):\n",
    "            data.x = F.one_hot(data.z, self.__max_z__ + 1).to(torch.float)\n",
    "\n",
    "            \n",
    "        print(self.collate(train_pos_list + train_neg_list))\n",
    "        torch.save(self.collate(train_pos_list + train_neg_list),\n",
    "                   self.processed_paths[0])\n",
    "        torch.save(self.collate(val_pos_list + val_neg_list),\n",
    "                   self.processed_paths[1])\n",
    "        torch.save(self.collate(test_pos_list + test_neg_list),\n",
    "                   self.processed_paths[2])\n",
    "\n",
    "    def extract_enclosing_subgraphs(self, link_index, edge_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in link_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "        \n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self.__max_z__ = max(int(z.max()), self.__max_z__)\n",
    "\n",
    "        return z.to(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "quick-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_list(x):\n",
    "    if not isinstance(x, (tuple, list)):\n",
    "        x = [x]\n",
    "    return x\n",
    "\n",
    "\n",
    "def files_exist(files):\n",
    "    return len(files) != 0 and all(osp.exists(f) for f in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "available-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, num_hops, root=None, split=\"train\"):\n",
    "        \n",
    "        self.num_hops = num_hops\n",
    "        super().__init__(root=root)\n",
    "        \n",
    "        index = ['train', 'valid'].index(split)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[index])\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        \n",
    "        return [\"train.csv\"]\n",
    "    \n",
    "#     @property\n",
    "#     def raw_dir(self):\n",
    "#         return os.path.join(self.root, \"hw2_data\", \"raw\")\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['SEAL_data_0.pt', 'SEAL_data_1.pt']\n",
    "    \n",
    "    def process(self):\n",
    "\n",
    "\n",
    "        # Read node features\n",
    "        content = pd.read_csv(os.path.join(self.raw_dir, \"content.csv\"), delimiter=\"\\t\", header=None)\n",
    "        content = content.sort_values(by=[0]).loc[:, 1:].to_numpy()\n",
    "        content = torch.from_numpy(content)\n",
    "        num_nodes = content.size(0)\n",
    "\n",
    "        # Read edge list\n",
    "        train = pd.read_csv(os.path.join(self.raw_dir, \"train.csv\"))\n",
    "\n",
    "        train_pos = train[ train[\"label\"] == 1]\n",
    "        train_neg = train[ train[\"label\"] == 0]\n",
    "\n",
    "        follower_pos = train_pos[\"from\"].to_numpy().tolist()\n",
    "        followee_pos = train_pos[\"to\"].to_numpy().tolist()\n",
    "        train_pos_edge = torch.tensor([follower_pos, followee_pos], dtype=torch.long)\n",
    "        train_pos_edge = to_undirected(train_pos_edge)\n",
    "#         train_pos_edge = add_self_loops(train_pos_edge)[0]\n",
    "\n",
    "        self.data = Data(x=content, edge_index=train_pos_edge, num_nodes=num_nodes)\n",
    "\n",
    "\n",
    "        train_pos_edge = train_pos_edge.t()\n",
    "\n",
    "        follower_neg = train_neg[\"from\"].to_numpy().tolist()\n",
    "        followee_neg = train_neg[\"to\"].to_numpy().tolist()\n",
    "        train_neg_edge = torch.tensor([follower_neg, followee_neg], dtype=torch.long)\n",
    "        train_neg_edge = to_undirected(train_neg_edge)\n",
    "#         train_neg_edge = add_self_loops(train_neg_edge)[0]\n",
    "        \n",
    "        train_neg_edge = train_neg_edge.t()\n",
    "\n",
    "        train_pos_edge, valid_pos_edge = train_test_split(train_pos_edge, shuffle=True)\n",
    "        train_neg_edge, valid_neg_edge = train_test_split(train_neg_edge, shuffle=True)\n",
    "\n",
    "        train_pos_edge = train_pos_edge.t()\n",
    "        valid_pos_edge = valid_pos_edge.t()\n",
    "        train_neg_edge = train_neg_edge.t()\n",
    "        valid_neg_edge = valid_neg_edge.t()\n",
    "\n",
    "\n",
    "        self.__max_z__ = 0\n",
    "\n",
    "\n",
    "        train_pos_list = self.extract_enclosing_subgraphs(\n",
    "            train_pos_edge, train_pos_edge, 1)\n",
    "        train_neg_list = self.extract_enclosing_subgraphs(\n",
    "            train_neg_edge, train_pos_edge, 0)\n",
    "        \n",
    "        print(train_pos_list)\n",
    "\n",
    "        val_pos_list = self.extract_enclosing_subgraphs(\n",
    "            valid_pos_edge, train_pos_edge, 1)\n",
    "        val_neg_list = self.extract_enclosing_subgraphs(\n",
    "            valid_neg_edge, train_pos_edge, 0)\n",
    "\n",
    "\n",
    "        # Convert labels to one-hot features.\n",
    "        for data in chain(train_pos_list, train_neg_list, \n",
    "                          val_pos_list, val_neg_list):\n",
    "            z = F.one_hot(data.z, self.__max_z__ + 1).to(torch.float)\n",
    "            data.x = torch.cat([z, data.x], 1)\n",
    "\n",
    "\n",
    "        \n",
    "            \n",
    "        torch.save(self.collate(train_pos_list + train_neg_list),\n",
    "                   self.processed_paths[0])\n",
    "        torch.save(self.collate(val_pos_list + val_neg_list),\n",
    "                   self.processed_paths[1])\n",
    "\n",
    "\n",
    "#         if self.pre_filter is not None and not self.pre_filter(data):\n",
    "#             continue\n",
    "\n",
    "#         if self.pre_transform is not None:\n",
    "#             data = self.pre_transform(data)\n",
    "\n",
    "\n",
    "#         torch.save(self.collate(train_pos_list + train_neg_list),\n",
    "#                    self.processed_paths[0])\n",
    "#         torch.save(self.collate(val_pos_list + val_neg_list),\n",
    "#                    self.processed_paths[1])\n",
    "\n",
    "    \n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "    \n",
    "    def extract_enclosing_subgraphs(self, link_index, edge_index, y):\n",
    "        data_list = []\n",
    "        for src, dst in link_index.t().tolist():\n",
    "            sub_nodes, sub_edge_index, mapping, _ = k_hop_subgraph(\n",
    "                [src, dst], self.num_hops, edge_index, relabel_nodes=True)\n",
    "            src, dst = mapping.tolist()\n",
    "\n",
    "            # Remove target link from the subgraph.\n",
    "            mask1 = (sub_edge_index[0] != src) | (sub_edge_index[1] != dst)\n",
    "            mask2 = (sub_edge_index[0] != dst) | (sub_edge_index[1] != src)\n",
    "            sub_edge_index = sub_edge_index[:, mask1 & mask2]\n",
    "\n",
    "            # Calculate node labeling.\n",
    "            z = self.drnl_node_labeling(sub_edge_index, src, dst,\n",
    "                                        num_nodes=sub_nodes.size(0))\n",
    "\n",
    "            data = Data(x=self.data.x[sub_nodes], z=z,\n",
    "                        edge_index=sub_edge_index, y=y)\n",
    "            data_list.append(data)\n",
    "\n",
    "        return data_list\n",
    "\n",
    "    def drnl_node_labeling(self, edge_index, src, dst, num_nodes=None):\n",
    "        # Double-radius node labeling (DRNL).\n",
    "        src, dst = (dst, src) if src > dst else (src, dst)\n",
    "        adj = to_scipy_sparse_matrix(edge_index, num_nodes=num_nodes).tocsr()\n",
    "\n",
    "        idx = list(range(src)) + list(range(src + 1, adj.shape[0]))\n",
    "        adj_wo_src = adj[idx, :][:, idx]\n",
    "\n",
    "        idx = list(range(dst)) + list(range(dst + 1, adj.shape[0]))\n",
    "        adj_wo_dst = adj[idx, :][:, idx]\n",
    "        \n",
    "#         print(src, adj_wo_dst.shape)\n",
    "#         print(adj_wo_dst[src])\n",
    "        \n",
    "\n",
    "        dist2src = shortest_path(adj_wo_dst, directed=False, unweighted=True,\n",
    "                                 indices=src)\n",
    "        dist2src = np.insert(dist2src, dst, 0, axis=0)\n",
    "        dist2src = torch.from_numpy(dist2src)\n",
    "\n",
    "        dist2dst = shortest_path(adj_wo_src, directed=False, unweighted=True,\n",
    "                                 indices=dst - 1)\n",
    "        dist2dst = np.insert(dist2dst, src, 0, axis=0)\n",
    "        dist2dst = torch.from_numpy(dist2dst)\n",
    "\n",
    "        dist = dist2src + dist2dst\n",
    "        dist_over_2, dist_mod_2 = dist // 2, dist % 2\n",
    "\n",
    "        z = 1 + torch.min(dist2src, dist2dst)\n",
    "        z += dist_over_2 * (dist_over_2 + dist_mod_2 - 1)\n",
    "        z[src] = 1.\n",
    "        z[dst] = 1.\n",
    "        z[torch.isnan(z)] = 0.\n",
    "\n",
    "        self.__max_z__ = max(int(z.max()), self.__max_z__)\n",
    "\n",
    "        return z.to(torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-shock",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_dataset = MyDataset(num_hops=2, root=os.path.join(os.getcwd(), \"hw2_data\", \"dataset1\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "adjustable-queen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 194], x=[73, 1518], y=[1], z=[73])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "public-highland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset[0].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "delayed-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(os.path.join(os.getcwd(), \"hw2_data\", \"raw\", \"dataset1\", \"content.csv\"), delimiter=\"\\t\", header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-thriller",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "traditional-examination",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DGCNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers, GNN=GCNConv, k=0.6):\n",
    "        super(DGCNN, self).__init__()\n",
    "\n",
    "        if k < 1:  # Transform percentile to number.\n",
    "            num_nodes = sorted([data.num_nodes for data in train_dataset])\n",
    "            k = num_nodes[int(math.ceil(k * len(num_nodes))) - 1]\n",
    "            k = max(10, k)\n",
    "        self.k = int(k)\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.convs.append(GNN(train_dataset.num_features, hidden_channels))\n",
    "        for i in range(0, num_layers - 1):\n",
    "            self.convs.append(GNN(hidden_channels, hidden_channels))\n",
    "        self.convs.append(GNN(hidden_channels, 1))\n",
    "\n",
    "        conv1d_channels = [16, 32]\n",
    "        total_latent_dim = hidden_channels * num_layers + 1\n",
    "        conv1d_kws = [total_latent_dim, 5]\n",
    "        self.conv1 = Conv1d(1, conv1d_channels[0], conv1d_kws[0],\n",
    "                            conv1d_kws[0])\n",
    "        self.maxpool1d = MaxPool1d(2, 2)\n",
    "        self.conv2 = Conv1d(conv1d_channels[0], conv1d_channels[1],\n",
    "                            conv1d_kws[1], 1)\n",
    "        dense_dim = int((self.k - 2) / 2 + 1)\n",
    "        dense_dim = (dense_dim - conv1d_kws[1] + 1) * conv1d_channels[1]\n",
    "        self.lin1 = Linear(dense_dim, 128)\n",
    "        self.lin2 = Linear(128, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        xs = [x]\n",
    "        for conv in self.convs:\n",
    "            xs += [torch.tanh(conv(xs[-1], edge_index))]\n",
    "        x = torch.cat(xs[1:], dim=-1)\n",
    "\n",
    "        # Global pooling.\n",
    "\n",
    "        x = global_sort_pool(x, batch, self.k)\n",
    "\n",
    "        x = x.unsqueeze(1)  # [num_graphs, 1, k * hidden]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.maxpool1d(x)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # [num_graphs, dense_dim]\n",
    "\n",
    "        \n",
    "        # MLP.\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "beautiful-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        data.edge_index = torch_geometric.utils.add_self_loops(data.edge_index)[0]\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        loss = BCEWithLogitsLoss()(logits.view(-1), data.y.to(torch.float))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "\n",
    "    return total_loss / len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "patient-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for data in loader:\n",
    "        data.edge_index = torch_geometric.utils.add_self_loops(data.edge_index)[0]\n",
    "        data = data.to(device)\n",
    "        logits = model(data.x, data.edge_index, data.batch)\n",
    "        y_pred.append(logits.view(-1).cpu())\n",
    "        y_true.append(data.y.view(-1).cpu().to(torch.float))\n",
    "\n",
    "    print(y_pred, y_true)\n",
    "    return roc_auc_score(torch.cat(y_true), torch.cat(y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-prague",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "apparent-colon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "intense-municipality",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n",
      "Processing...\n",
      "(Data(edge_index=[2, 3657788], x=[1152984, 136], y=[17952], z=[1152984]), {'x': tensor([      0,       8,      74,  ..., 1152855, 1152937, 1152984]), 'edge_index': tensor([      0,      14,     160,  ..., 3657430, 3657662, 3657788]), 'y': tensor([    0,     1,     2,  ..., 17950, 17951, 17952]), 'z': tensor([      0,       8,      74,  ..., 1152855, 1152937, 1152984])})\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cora = Planetoid(root=os.getcwd(), name='Cora')\n",
    "\n",
    "train_dataset = SEALDataset(cora, num_hops=2, split='train')\n",
    "val_dataset = SEALDataset(cora, num_hops=2, split='val')\n",
    "test_dataset = SEALDataset(cora, num_hops=2, split='test')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "clean-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 14], x=[8, 136], y=[1], z=[8])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "resistant-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bored-bedroom",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 0.5306, Val: 0.8924, Test: 0.8749\n",
      "Epoch: 02, Loss: 0.4207, Val: 0.9190, Test: 0.9064\n",
      "Epoch: 03, Loss: 0.3860, Val: 0.9214, Test: 0.9157\n",
      "Epoch: 04, Loss: 0.3690, Val: 0.9252, Test: 0.9219\n",
      "Epoch: 05, Loss: 0.3618, Val: 0.9281, Test: 0.9259\n",
      "Epoch: 06, Loss: 0.3556, Val: 0.9291, Test: 0.9302\n",
      "Epoch: 07, Loss: 0.3511, Val: 0.9305, Test: 0.9303\n",
      "Epoch: 08, Loss: 0.3480, Val: 0.9290, Test: 0.9303\n",
      "Epoch: 09, Loss: 0.3465, Val: 0.9298, Test: 0.9303\n",
      "Epoch: 10, Loss: 0.3455, Val: 0.9309, Test: 0.9285\n",
      "Epoch: 11, Loss: 0.3428, Val: 0.9325, Test: 0.9323\n",
      "Epoch: 12, Loss: 0.3423, Val: 0.9316, Test: 0.9323\n",
      "Epoch: 13, Loss: 0.3421, Val: 0.9335, Test: 0.9334\n",
      "Epoch: 14, Loss: 0.3423, Val: 0.9265, Test: 0.9334\n",
      "Epoch: 15, Loss: 0.3401, Val: 0.9344, Test: 0.9339\n",
      "Epoch: 16, Loss: 0.3389, Val: 0.9352, Test: 0.9342\n",
      "Epoch: 17, Loss: 0.3385, Val: 0.9325, Test: 0.9342\n",
      "Epoch: 18, Loss: 0.3380, Val: 0.9322, Test: 0.9342\n",
      "Epoch: 19, Loss: 0.3365, Val: 0.9339, Test: 0.9342\n",
      "Epoch: 20, Loss: 0.3368, Val: 0.9343, Test: 0.9342\n",
      "Epoch: 21, Loss: 0.3365, Val: 0.9361, Test: 0.9321\n",
      "Epoch: 22, Loss: 0.3366, Val: 0.9345, Test: 0.9321\n",
      "Epoch: 23, Loss: 0.3352, Val: 0.9358, Test: 0.9321\n",
      "Epoch: 24, Loss: 0.3342, Val: 0.9346, Test: 0.9321\n",
      "Epoch: 25, Loss: 0.3362, Val: 0.9357, Test: 0.9321\n",
      "Epoch: 26, Loss: 0.3349, Val: 0.9346, Test: 0.9321\n",
      "Epoch: 27, Loss: 0.3341, Val: 0.9346, Test: 0.9321\n",
      "Epoch: 28, Loss: 0.3339, Val: 0.9367, Test: 0.9324\n",
      "Epoch: 29, Loss: 0.3327, Val: 0.9354, Test: 0.9324\n",
      "Epoch: 30, Loss: 0.3330, Val: 0.9380, Test: 0.9339\n",
      "Epoch: 31, Loss: 0.3321, Val: 0.9350, Test: 0.9339\n",
      "Epoch: 32, Loss: 0.3314, Val: 0.9366, Test: 0.9339\n",
      "Epoch: 33, Loss: 0.3319, Val: 0.9362, Test: 0.9339\n",
      "Epoch: 34, Loss: 0.3329, Val: 0.9376, Test: 0.9339\n",
      "Epoch: 35, Loss: 0.3331, Val: 0.9377, Test: 0.9339\n",
      "Epoch: 36, Loss: 0.3305, Val: 0.9386, Test: 0.9341\n",
      "Epoch: 37, Loss: 0.3301, Val: 0.9364, Test: 0.9341\n",
      "Epoch: 38, Loss: 0.3320, Val: 0.9359, Test: 0.9341\n",
      "Epoch: 39, Loss: 0.3318, Val: 0.9370, Test: 0.9341\n",
      "Epoch: 40, Loss: 0.3308, Val: 0.9360, Test: 0.9341\n",
      "Epoch: 41, Loss: 0.3309, Val: 0.9368, Test: 0.9341\n",
      "Epoch: 42, Loss: 0.3316, Val: 0.9363, Test: 0.9341\n",
      "Epoch: 43, Loss: 0.3296, Val: 0.9371, Test: 0.9341\n",
      "Epoch: 44, Loss: 0.3309, Val: 0.9335, Test: 0.9341\n",
      "Epoch: 45, Loss: 0.3302, Val: 0.9330, Test: 0.9341\n",
      "Epoch: 46, Loss: 0.3316, Val: 0.9348, Test: 0.9341\n",
      "Epoch: 47, Loss: 0.3307, Val: 0.9349, Test: 0.9341\n",
      "Epoch: 48, Loss: 0.3301, Val: 0.9330, Test: 0.9341\n",
      "Epoch: 49, Loss: 0.3284, Val: 0.9332, Test: 0.9341\n",
      "Epoch: 50, Loss: 0.3299, Val: 0.9330, Test: 0.9341\n"
     ]
    }
   ],
   "source": [
    "best_val_auc = test_auc = 0\n",
    "for epoch in range(1, 51):\n",
    "    loss = train()\n",
    "    val_auc = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "        test_auc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_auc:.4f}, '\n",
    "          f'Test: {test_auc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesbian-ensemble",
   "metadata": {},
   "source": [
    "# My Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-river",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "appointed-monte",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    1,  ..., 2707, 2707, 2707],\n",
       "        [ 563, 2664,  962,  ..., 1981, 2657, 2695]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-secretariat",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(num_hops=2, root=os.path.join(os.getcwd(), \"hw2_data\", \"dataset1\"), split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "particular-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset =  MyDataset(num_hops=2, root=os.path.join(os.getcwd(), \"hw2_data\", \"dataset1\"), split=\"train\")\n",
    "val_dataset =  MyDataset(num_hops=2, root=os.path.join(os.getcwd(), \"hw2_data\", \"dataset1\"), split=\"valid\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "packed-mailing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "copyrighted-ordinary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 194], x=[73, 1518], y=[1], z=[73])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "subjective-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "model = DGCNN(hidden_channels=32, num_layers=3).to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "democratic-modern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([0.0335, 0.0322])] [tensor([1., 1.])]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Only one class present in y_true. ROC AUC score is not defined in that case.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-8fc67fb3c746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mval_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mval_auc\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbest_val_auc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mbest_val_auc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_auc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/june/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-25ab7b241f02>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/june/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/june/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    543\u001b[0m                                              max_fpr=max_fpr),\n\u001b[1;32m    544\u001b[0m                                      \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m                                      sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    546\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# multilabel-indicator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         return _average_binary_score(partial(_binary_roc_auc_score,\n",
      "\u001b[0;32m~/anaconda3/envs/june/lib/python3.7/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbinary_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/june/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_roc_auc_score\u001b[0;34m(y_true, y_score, sample_weight, max_fpr)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;34m\"\"\"Binary roc auc score.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         raise ValueError(\"Only one class present in y_true. ROC AUC score \"\n\u001b[0m\u001b[1;32m    328\u001b[0m                          \"is not defined in that case.\")\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Only one class present in y_true. ROC AUC score is not defined in that case."
     ]
    }
   ],
   "source": [
    "best_val_auc = test_auc = 0\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    val_auc = test(val_loader)\n",
    "    if val_auc > best_val_auc:\n",
    "        best_val_auc = val_auc\n",
    "#         test_auc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Val: {val_auc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advised-polyester",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
